<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>TED-SMPLX</title><meta name="csrf-param" content="authenticity_token"><meta name="csrf-token" content="HguP2affJzIDogcJFPelIDPZWZXa8ik7o9B4CpjxsOGtvHOiYV1x90Rn2g6KAUxewvU7uHrfPLhy5B0ky/ThTg=="><meta name="turbolinks-cache-control" content="no-cache"><meta name="keywords" content=""><meta name="description" content=""><meta name="author" content="Jonathan Williams"> <meta property="og:title" content="3d ARGA"> <meta property="og:description" content=""> <meta property="og:type" content="website"> <meta property="og:url" content="https://andrewfengusa.github.io/TED_SMPLX_Dataset/"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
<style type="text/css">svg:not(:root).svg-inline--fa{overflow:visible}.svg-inline--fa{display:inline-block;font-size:inherit;height:1em;overflow:visible;vertical-align:-.125em}.svg-inline--fa.fa-lg{vertical-align:-.225em}.svg-inline--fa.fa-w-1{width:.0625em}.svg-inline--fa.fa-w-2{width:.125em}.svg-inline--fa.fa-w-3{width:.1875em}.svg-inline--fa.fa-w-4{width:.25em}.svg-inline--fa.fa-w-5{width:.3125em}.svg-inline--fa.fa-w-6{width:.375em}.svg-inline--fa.fa-w-7{width:.4375em}.svg-inline--fa.fa-w-8{width:.5em}.svg-inline--fa.fa-w-9{width:.5625em}.svg-inline--fa.fa-w-10{width:.625em}.svg-inline--fa.fa-w-11{width:.6875em}.svg-inline--fa.fa-w-12{width:.75em}.svg-inline--fa.fa-w-13{width:.8125em}.svg-inline--fa.fa-w-14{width:.875em}.svg-inline--fa.fa-w-15{width:.9375em}.svg-inline--fa.fa-w-16{width:1em}.svg-inline--fa.fa-w-17{width:1.0625em}.svg-inline--fa.fa-w-18{width:1.125em}.svg-inline--fa.fa-w-19{width:1.1875em}.svg-inline--fa.fa-w-20{width:1.25em}.svg-inline--fa.fa-pull-left{margin-right:.3em;width:auto}.svg-inline--fa.fa-pull-right{margin-left:.3em;width:auto}.svg-inline--fa.fa-border{height:1.5em}.svg-inline--fa.fa-li{width:2em}.svg-inline--fa.fa-fw{width:1.25em}.fa-layers svg.svg-inline--fa{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.fa-layers{display:inline-block;height:1em;position:relative;text-align:center;vertical-align:-.125em;width:1em}.fa-layers svg.svg-inline--fa{-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter,.fa-layers-text{display:inline-block;position:absolute;text-align:center}.fa-layers-text{left:50%;top:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter{background-color:#ff253a;border-radius:1em;-webkit-box-sizing:border-box;box-sizing:border-box;color:#fff;height:1.5em;line-height:1;max-width:5em;min-width:1.5em;overflow:hidden;padding:.25em;right:0;text-overflow:ellipsis;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-bottom-right{bottom:0;right:0;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom right;transform-origin:bottom right}.fa-layers-bottom-left{bottom:0;left:0;right:auto;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom left;transform-origin:bottom left}.fa-layers-top-right{right:0;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-top-left{left:0;right:auto;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top left;transform-origin:top left}.fa-lg{font-size:1.33333em;line-height:.75em;vertical-align:-.0667em}.fa-xs{font-size:.75em}.fa-sm{font-size:.875em}.fa-1x{font-size:1em}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-6x{font-size:6em}.fa-7x{font-size:7em}.fa-8x{font-size:8em}.fa-9x{font-size:9em}.fa-10x{font-size:10em}.fa-fw{text-align:center;width:1.25em}.fa-ul{list-style-type:none;margin-left:2.5em;padding-left:0}.fa-ul>li{position:relative}.fa-li{left:-2em;position:absolute;text-align:center;width:2em;line-height:inherit}.fa-border{border:solid .08em #eee;border-radius:.1em;padding:.2em .25em .15em}.fa-pull-left{float:left}.fa-pull-right{float:right}.fa.fa-pull-left,.fab.fa-pull-left,.fal.fa-pull-left,.far.fa-pull-left,.fas.fa-pull-left{margin-right:.3em}.fa.fa-pull-right,.fab.fa-pull-right,.fal.fa-pull-right,.far.fa-pull-right,.fas.fa-pull-right{margin-left:.3em}.fa-spin{-webkit-animation:fa-spin 2s infinite linear;animation:fa-spin 2s infinite linear}.fa-pulse{-webkit-animation:fa-spin 1s infinite steps(8);animation:fa-spin 1s infinite steps(8)}@-webkit-keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.fa-rotate-90{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{-webkit-transform:scale(-1,1);transform:scale(-1,1)}.fa-flip-vertical{-webkit-transform:scale(1,-1);transform:scale(1,-1)}.fa-flip-horizontal.fa-flip-vertical{-webkit-transform:scale(-1,-1);transform:scale(-1,-1)}:root .fa-flip-horizontal,:root .fa-flip-vertical,:root .fa-rotate-180,:root .fa-rotate-270,:root .fa-rotate-90{-webkit-filter:none;filter:none}.fa-stack{display:inline-block;height:2em;position:relative;width:2em}.fa-stack-1x,.fa-stack-2x{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.svg-inline--fa.fa-stack-1x{height:1em;width:1em}.svg-inline--fa.fa-stack-2x{height:2em;width:2em}.fa-inverse{color:#fff}.sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.sr-only-focusable:active,.sr-only-focusable:focus{clip:auto;height:auto;margin:0;overflow:visible;position:static;width:auto}</style><link rel="stylesheet" media="all" href="./IVA_files/default.css" data-turbolinks-track="reload">

<style>.cke{visibility:hidden;}</style></head><body data-new-gr-c-s-check-loaded="14.1058.0" data-gr-ext-installed=""><div class="d-block d-lg-none"><nav id="menu" class="menu slideout-menu slideout-menu-right"> <section class="menu-section"><ul class="menu-section-list"><li class="mobile-menu-breaker-bottom"><a class="page-scroll" href="https://andrewfengusa.github.io/TED_SMPLX_Dataset/">Home</a></li><li class="mobile-menu-breaker-bottom"><a class="page-scroll" href="./IVA_files/news.html">News</a></li><li class="mobile-menu-breaker-bottom"><a class="page-scroll" href="https://forms.gle/kMDegwtpicMHH31y8">Downloads</a></li><li class="mobile-menu-breaker-bottom"><a class="page-scroll" href="./IVA_files/license.html">License</a></li><li class="mobile-menu-breaker-bottom"><a class="page-scroll" href="./IVA_files/extra.html">Extra</a></li></ul></section></nav></div><header class="header-wrapper fixed"><div class="fixed-top"><nav id="navbar" class="navbar navbar-expand-md navbar-light bg-light ewt-navbar "><div class="container"><a href="https://andrewfengusa.github.io/TED_SMPLX_Dataset/" id="header-logo" class="d-none d-lg-block d-xl-block navbar-brand header-logo-text no-link ">TED-SMPLX</a><a href="https://andrewfengusa.github.io/TED_SMPLX_Dataset/" id="header-logo" class="d-block d-lg-none navbar-brand header-logo-text no-link">TED-SMPLX</a><div class="navbar-collapse ewt-navbar-collapse" id="navbarCollapse"><div class="ml-auto d-none d-lg-block">
<!--    <ul class="navbar-nav ml-auto"><li id="nav-item" class="nav-item"><a class="nav-link" href="https://samuel3shin.github.io">Home</a></li><li id="nav-item" class="nav-item"><a class="nav-link" href="./IVA_files/news.html">News</a></li><li id="nav-item" class="nav-item"><a class="nav-link" href="https://forms.gle/kMDegwtpicMHH31y8">Downloads</a></li><li id="nav-item" class="nav-item"><a class="nav-link" href="./IVA_files/license.html">License</a></li><li id="nav-item" class="nav-item"><a class="nav-link" href="./IVA_files/extra.html">Extra</a></li>-->
<!--</ul>-->
</div><ul class="nav ml-auto d-lg-none pull-right"><li><button type="button" class="js-slideout-toggle btn btn-mobile-menu"><svg class="svg-inline--fa fa-bars fa-w-14" aria-hidden="true" data-prefix="fas" data-icon="bars" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" data-fa-i2svg=""><path fill="currentColor" d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg><!-- <i class="fas fa-bars"></i> --></button></li></ul> </div></div></nav></div></header><main id="main" class="panel main-panel slideout-panel slideout-panel-right"><div class="section-top d-lg-none"></div><div class="section section-top section-no-padding-top section-no-padding-bottom" style=""><div class="container content-link"></div></div><div class="section section-no-padding-top section-no-padding-bottom" style=""><div class="container content-link"><h1 style="text-align: center;"><strong>TED-SMPLX </strong></h1>
<h1 style="text-align: center;">A Tool for Extracting 3D Avatar-Ready Gesture Animations from Monocular Videos</h1>
<p style="text-align: center;"><a href="https://github.com/andrewfengusa">Andrew Feng</a>, <a href="https://github.com/Samuel3Shin">Samuel Shin</a>, <a href="https://github.com/youngwoo-yoon">Youngwoo Yoon</a></p>
<p style="text-align: center;"><span style="font-size: 18pt;">MIG 2022</span></p>
<p style="text-align: center;"><span style="font-size: 24pt;"><span style="font-size: 18pt;"><span style="font-size: 14pt;">[<a href="https://drive.google.com/file/d/1OXOKRiyam2QcO9vivOSZ1ARdBfNprcqj/view?usp=sharing">Paper</a>]&nbsp; [<a href="https://youtu.be/nmef_FUavzU">Video</a>]&nbsp; [<a href="https://github.com/andrewfengusa/VideoMotionExtractor">Code</a>]&nbsp;</span></span></span> </p>
<p style="text-align: center;"><img src="./IVA_files/header_medium.png" class="img-fluid" alt="Responsive image"></p>
<p style="text-align: center;"><span style="font-size: 18pt;">3D Avatar-Ready Gesture Animations from Monocular Videos</span></p></div></div><div class="section section-no-padding-top section-no-padding-bottom" style=""><div class="container content-link"><hr>
<h3 style="text-align: center;"><strong>Abstract</strong></h3>
<p style="text-align: center;">Modeling and generating realistic human gesture animations from speech audios has great impacts on creating a believable virtual human that can interact with human users and mimic real-world face-to-face communications. Large-scale datasets are essential in data-driven research, but creating multi-modal gesture datasets with 3D gesture motions and corresponding speech audios is either expensive to create via traditional workflow such as mocap, or producing subpar results via pose estimations from in-the-wild videos. As a result of such limitations, existing gesture datasets either suffer from shorter duration or lower animation quality, making them less ideal for training gesture synthesis models. Motivated by the key limitations from previous datasets and recent progress in human mesh recovery (HMR), we developed a tool for extracting avatar-ready gesture motions from monocular videos with improved animation quality. The tool utilizes a variational autoencoder (VAE) to refine raw gesture motions. The resulting gestures are in a unified pose representation that includes both body and finger motions and can be readily applied to a virtual avatar via online motion retargeting. We validated the proposed tool on existing datasets and created the refined dataset TED-SMPLX by re-processing videos from the original TED dataset. The new dataset will be made available for future research. Samples showing the extracted gesture motion can be found in the video link at https://youtu.be/nmef_FUavzU.</p>
<!--<p style="text-align: center;"><iframe allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" height="472" src="https://www.youtube.com/embed/NubxhF1xNhY?autoplay=1" width="840"></iframe></p> --></div></div><div class="section section-no-padding-top section-no-padding-bottom" style=""><div class="container content-link"><h1 style="text-align: center;"><strong>Video</strong></h1>
<div class="embed-responsive embed-responsive-16by9">
    <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/nmef_FUavzU" allowfullscreen></iframe>
</div><hr>
    <!-- <div style="position: relative; height: 0; padding-bottom: 56.25%;"><iframe style="position: absolute;" title="YouTube video player" src="./IVA_files/cceRrlnTCEs.html" width="100%" height="100%" frameborder="0" allowfullscreen="allowfullscreen"></iframe></div></div></div><div class="section section-no-padding-top section-no-padding-bottom" style=""><div class="container content-link"><hr> -->
<h3 style="text-align: center;"><strong>Download</strong></h3>
<!-- <p>We provide the body model parameters corresponding to each motion capture sequence in the included datasets, along with tutorial code to visualiziation and simple application in deep learning tasks.</p> -->
<p style="text-align: center;">We provide the body model parameters corresponding to each motion capture sequence in the included datasets, along with <a href="https://github.com/andrewfengusa/SMPLX_Visualizer.git">tutorial code</a> to visualize the data and basic tools to use it in deep learning tasks.</p>
<p style="text-align: center;">You can&nbsp;request download&nbsp;<a href="https://forms.gle/Fm1aE8hnHe7dXTgS7">here</a>.</p></div></div><div class="section section-no-padding-top section-no-padding-bottom" style=""><div class="container content-link"><hr>
<h3 style="text-align: center;"><strong>Referencing the TED-SMPLX Dataset</strong></h3>
<div class="card code-card">
<div class="card-body">
<pre>@conference{ted_smplx:MIG:2022,
&nbsp; title = {A Tool for Extracting 3D Avatar-Ready Gesture Animations from Monocular Videos},
&nbsp; author = {Andrew Feng, Samuel Shin, Youngwoo Yoon},
&nbsp; booktitle = {the Name of Booktitle},
&nbsp; pages = {0000--0000},
&nbsp; month = may,
&nbsp; year = {2022},
&nbsp; month_numeric = {00}
}</pre>
</div>
</div>
<strong>Contact</strong>
<p style="text-align: center;">If you have any questions or comments regarding the dataset, please contact Andrew Feng (<a href="mailto:feng@ict.usc.edu?subject=Inquire%20About%203DARGA%20Dataset">feng@ict.usc.edu</a>).</p>
<p><small>Created by <a href="https://ict.usc.edu/" target="_blank" rel="noopener">USC Institute for Creative Technologies</a>.</small></p></div></div><div class="d-none d-sm-block"><div class="section-footer-bottom section-border-top section-footer-ps-sub-bottom"><div class="container"><div class="row"><div class="col-md-8 content-link"><small>© 2022 USC Institute for Creative Technologies<small class="text-muted ml-1 mr-1">-</small><a href="./IVA_files/imprint.html">Imprint</a><small class="text-muted ml-1 mr-1">-</small><a href="./IVA_files/license.html">License</a></small></div>
</div></div></div></div><div class="d-block d-sm-none"><div class="section-footer-bottom section-border-top section-footer-ps-sub-bottom"><div class="container"><div class="row"><div class="col-md-8 content-link"><small>© 2022 USC Institute for Creative Technology </small></div></div></div></div></div></main>
<script src="./IVA_files/jquery-3.5.1.slim.min.js" data-turbolinks-track="reload"></script>
<script src="./IVA_files/default.js" data-turbolinks-track="reload"></script>

<div class="modal fade" id="remote-conent-modal" tabindex="-1" role="dialog" aria-labelledby="exampleModalCenterTitle" aria-hidden="true"><div class="modal-dialog modal-preview modal-dialog-centered" role="document"><div class="modal-content"></div></div></div>
</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>